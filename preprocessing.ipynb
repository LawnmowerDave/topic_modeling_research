{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:52.342511Z",
     "start_time": "2018-05-07T11:29:50.385716Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "'''\n",
    "Though not explicitly stated, I believe this \"stopwords\" package originates\n",
    "from this pip package: https://pypi.org/project/stop-words/\n",
    "\n",
    "It was the first result I found that shares the same name and get_stop_words\n",
    "function call. \n",
    "'''\n",
    "import stop_words\n",
    "from dateutil import parser\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# don't condense large numbers to scientific notation\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "filenames = glob(\"data/BTC/*.csv\")\n",
    "stopwords = set(stop_words.get_stop_words('en'))\n",
    "stopwords.update(['quote', 'pmquote', 'amquote', 'just', 'don', 'one', 'thing', 'even', 'way', 'maybe', 'also', 'please', 'well', 'actually', 'something',\n",
    "                                         'going', 'anything', 'le', 'ever', 'say', 'see', 'likely', 'per', 'another', 'someone', 'let', 'anyone', 'doesn', 'include', 'doe', 'exactly',\n",
    "                                         'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'like',\n",
    "                                         'said', 'guy', 'will', 'can', 'able', 'people', 'become', 'tell', 'hey', 'much', 'many', 'lol', 'lot', 'want', 'still', 'really', 'think', 'didn',\n",
    "                                         'isn', 'post', 'edited', 'share', 'facebookshare', 'twitter'])\n",
    "                                        # adding exclusions to 'bitcion' because presumably, the entire dataset contains\n",
    "                                        # bitcoin-related tweets, which means mentions of bitcoin shouldn't add any value\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:54.235846Z",
     "start_time": "2018-05-07T11:29:54.225009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Current preprocessing protocol\n",
    "\n",
    "Remove:\n",
    "Hyperlinks\n",
    "Characters not in the English alphabetical character set a-z or A-Z\n",
    "stopwords\n",
    "words less than 3 characters\n",
    "'bitcoins' and replace with 'bitcoin'\n",
    "\n",
    "'''\n",
    "def parse_string(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    # remove links\n",
    "    input_string = re.sub(r'http\\S+', ' ', input_string)\n",
    "    input_string = re.sub(r'\\S+.(com|org)', '', input_string)\n",
    "    # remove all non-English alphabet characters including numbers,\n",
    "    # foreign and special characters\n",
    "    input_string = re.sub( \"[^a-zA-Z]\", \" \", input_string).split()\n",
    "\n",
    "    # lemmatize word\n",
    "    words = [lemmatizer.lemmatize(w) for w in input_string]\n",
    "    # get rid of stopwords and words less than 3 characters\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "\n",
    "    # fix common misspellings of bitcoin\n",
    "    words = [\"bitcoin\" if w == \"bitcoins\" else w for w in words]\n",
    "    words = [\"bitcoin\" if w == \"itcoin\" else w for w in words]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:37:28.909112Z",
     "start_time": "2018-05-07T11:33:15.478609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe size 4326298\n",
      "Number of edges 823069\n",
      "Number of unique nodes 296975\n",
      "Nodes de-duplicated 3206254\n",
      "De-duplication percentage 91.52%\n"
     ]
    }
   ],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "\n",
    "# a dictionary of user's posts, their time, quantity\n",
    "# accessed via preprocessed_data[field][user_name]\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts_num'] = defaultdict(int)\n",
    "preprocessed_data['get_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['write_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n",
    "\n",
    "\n",
    "fields = ['user_name', 'created_at', 'text', 'id', 'in_reply_to_screen_name']\n",
    "\n",
    "df = pd.concat([pd.read_csv(filename, nrows=None, usecols=fields) for filename in filenames])\n",
    "\n",
    "# drop null usernames and text\n",
    "df = df[df['user_name'].notna()]\n",
    "df = df[df['created_at'].notna()]\n",
    "df = df[df['text'].notna()]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.sort_values(\"user_name\", inplace=True)\n",
    "\n",
    "# twitter usernames are not case sensitive so convert everything to lowercase\n",
    "df['user_name'] = df['user_name'].str.lower()\n",
    "df['in_reply_to_screen_name'] = df['in_reply_to_screen_name'].str.lower()\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "if len(df[df.index.duplicated()]) > 0:\n",
    "    print(\"duplicate indices found! Something went wrong.\")\n",
    "\n",
    "orig_n_edges = len(df[df['in_reply_to_screen_name'].notna()])\n",
    "orig_n_nodes = len(df[df['in_reply_to_screen_name'].isna()]['user_name'].unique())\n",
    "orig_n_posts = len(df[df['in_reply_to_screen_name'].isna()])\n",
    "n_deduplicated = len(df) - (orig_n_edges + orig_n_nodes)\n",
    "\n",
    "print(f\"dataframe size {len(df)}\")\n",
    "print(f\"Number of edges {orig_n_edges}\")\n",
    "print(f\"Number of unique nodes {orig_n_nodes}\")\n",
    "# show the number of nodes that were not double counted as a result of using 'unique'\n",
    "print(f\"Nodes de-duplicated {n_deduplicated}\")\n",
    "# percentage of nodes removed. 0% would mean every user only has one post. \n",
    "# This number trends towards 100% as the ratio of posts to users increases\n",
    "print(f\"De-duplication percentage {100 * n_deduplicated / orig_n_posts:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 11.796376799393888\n",
      "median 1.0\n",
      "mode 1\n",
      "\n",
      "min 1\n",
      "max 43886\n"
     ]
    }
   ],
   "source": [
    "# show counts for number of posts for each user, some range in the thousands\n",
    "post_freq_dist = df[df['in_reply_to_screen_name'].isna()].groupby('user_name').count().sort_values('id')[\"index\"]\n",
    "\n",
    "# it appears that most nodes are identified by a single post\n",
    "print(f\"mean {post_freq_dist.mean()}\")\n",
    "print(f\"median {post_freq_dist.median()}\")\n",
    "print(f\"mode {post_freq_dist.mode()[0]}\")\n",
    "print()\n",
    "print(f\"min {post_freq_dist.min()}\")\n",
    "print(f\"max {post_freq_dist.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% done\n",
      "\n",
      "Number of commented nodes 2771\n",
      "\n",
      "Number of commented posts 118474\n",
      "Number of edges 52963\n",
      "\n",
      "Total node utilization 0.93%\n",
      "Total post utilization 3.38%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Construct a new dataframe from the existing data in order to construct our digraph. The old\n",
    "code assumes that for each post, there is a number of comments directly linked to that post.\n",
    "For this dataset, however, each comment is linked to a user, which is still enough info to \n",
    "construct the graph, but does not allow us to process it in the same manner. \n",
    "\n",
    "The code below, therefore will link together user_name and a set of comments. \n",
    "Importantly, if the user the comments are directed towards doesn't exist, their total \n",
    "comments will be zero, as we can derive nothing from them. \n",
    "'''\n",
    "\n",
    "user_comments_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# create dictionary which maps usernames to a set of comment indices\n",
    "# start by looping through comments\n",
    "for index, user_name in df[\"in_reply_to_screen_name\"].items(): \n",
    "\n",
    "    if counter % 10000 == 0:\n",
    "        print(f\"\\r{counter/len(df) * 100:.2f}% done\", end='')\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    # skip nan values\n",
    "    if type(user_name) == float:\n",
    "        continue\n",
    "\n",
    "    user_loc = df['user_name'].searchsorted(user_name)\n",
    "\n",
    "    # the user this comment is pointing to doesn't have a post\n",
    "    if not(user_name in df.iloc[user_loc]['user_name']):\n",
    "        continue\n",
    "\n",
    "    if user_name in user_comments_dict:\n",
    "        user_comments_dict[user_name].append(df.loc[index])\n",
    "    #  user has comments, init and add it here\n",
    "    else:\n",
    "        user_comments_dict[user_name] = [df.loc[index]]\n",
    "\n",
    "\n",
    "\n",
    "# drop all users without any comments pointing to them\n",
    "df = df[df['user_name'].isin(user_comments_dict.keys())]\n",
    "\n",
    "# remove comments, as they're already listed in the user comments dict\n",
    "df = df[df['in_reply_to_screen_name'].isna()]\n",
    "\n",
    "n_commented_nodes = len(user_comments_dict)\n",
    "n_commented_posts = sum(df.groupby('user_name').count().sort_values('id')[\"index\"])\n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"Number of commented nodes {n_commented_nodes}\")\n",
    "print()\n",
    "print(f\"Number of commented posts {n_commented_posts}\")\n",
    "print(f\"Number of edges {sum([len(vals) for vals in user_comments_dict.values()])}\")\n",
    "print()\n",
    "print(f\"Total node utilization {100 * n_commented_nodes / orig_n_nodes:.2f}%\")\n",
    "print(f\"Total post utilization {100 * n_commented_posts / orig_n_posts:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts: 100.00% done\n",
      "comments: 100.00% done\n",
      "Number of unique words: 50481\n"
     ]
    }
   ],
   "source": [
    "# word_freq_file_name = 'pkl/word_freq_pkl' \n",
    "\n",
    "# # load cached result. Important: Make sure the number of tweets used is the same in the df!\n",
    "# if os.path.exists(word_freq_file_name):\n",
    "#     with open(word_freq_file_name, 'rb') as f:\n",
    "#         word_freq = pickle.load(f)\n",
    "# else:\n",
    "\n",
    "#     with open(word_freq_file_name, 'wb') as f:\n",
    "#         pickle.dump(word_freq, f)\n",
    "\n",
    "# update word count\n",
    "for i, text in enumerate(df[\"text\"]):\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"\\rposts: {i/len(df) * 100:.2f}% done\", end='')\n",
    "        \n",
    "    text_body = parse_string(text)\n",
    "    word_freq.update(text_body)\n",
    "\n",
    "print(f\"\\rposts: {100:.2f}% done\", end='')\n",
    "print()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for comments in user_comments_dict.values():\n",
    "    if i % 3000 == 0:\n",
    "        print(f\"\\rcomments: {i/len(user_comments_dict) * 100:.2f}% done\", end='')\n",
    "        \n",
    "    for comment in comments:\n",
    "        word_freq.update(parse_string(comment.text))\n",
    "        i += 1\n",
    "\n",
    "print(f\"\\rcomments: {100:.2f}% done\", end='')\n",
    "print()\n",
    "print(\"Number of unique words:\", len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% done\n",
      "pickling...\n",
      "Number of unique vocabulary words 8984\n",
      "Removed 8341 posts\n",
      "Removed 7217 comments\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "user_processed_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "voca = set()\n",
    "\n",
    "min_post_len = 5\n",
    "min_comment_len = 5\n",
    "min_word_freq = 10\n",
    "\n",
    "# counters for how many posts were removed from each source\n",
    "n_removed_posts = 0\n",
    "n_removed_comments = 0\n",
    "\n",
    "# only loop through posts. Comments will be counted if the user is found\n",
    "# to be in the comments_user_dict \n",
    "for row in df.itertuples():\n",
    "\n",
    "    # absolute index, not implicit\n",
    "    index = row.Index\n",
    "\n",
    "    if counter % 5000 == 0:\n",
    "        print(f\"\\r{counter/len(df) * 100:.2f}% done\", end='')\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    post_body = parse_string(row.text)\n",
    "    post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "    \n",
    "    # skip posts of four words or less\n",
    "    if len(post_body) < min_post_len:\n",
    "        n_removed_posts += 1\n",
    "        continue\n",
    "\n",
    "    voca.update(post_body)\n",
    "    post_user = row.user_name\n",
    "    posted_time = parser.parse(row.created_at).date()\n",
    "    \n",
    "    preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "    preprocessed_data['user_posts_num'][post_user] += 1\n",
    "    preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "\n",
    "    # also add the same post data to a dictionary accessible by\n",
    "    # the user_name and time\n",
    "    if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "    else:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "    \n",
    "    preprocessed_data['posts'].append(post_body)\n",
    "\n",
    "    # comments already processed for this user\n",
    "    if post_user in user_processed_dict:\n",
    "        continue\n",
    "\n",
    "    # loop through the comments for the post and construct the digraph\n",
    "    for comment in user_comments_dict[post_user]:\n",
    "\n",
    "        user_processed_dict[post_user] = True\n",
    "\n",
    "        # skip comments on own post\n",
    "        if comment.user_name == post_user:\n",
    "            continue\n",
    "\n",
    "        comment_body = parse_string(comment.text)\n",
    "        comment_body = [w for w in comment_body if word_freq[w] >= min_word_freq]\n",
    "        \n",
    "        if len(comment_body) < min_comment_len:\n",
    "            n_removed_comments += 1\n",
    "            continue\n",
    "\n",
    "        voca.update(comment_body)\n",
    "        comment_user = comment.user_name\n",
    "        comment_time = parser.parse(comment.created_at).date()\n",
    "            \n",
    "        preprocessed_data['user_posts'][comment_user].append(comment_body)\n",
    "        preprocessed_data['time_posts'][comment_time].append(comment_body)\n",
    "\n",
    "        if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "        else:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "        \n",
    "        preprocessed_data['posts'].append(comment_body)\n",
    "        preprocessed_data['user_network'].add_edge(comment_user, post_user)\n",
    "        preprocessed_data['get_comment_num'][post_user] = len(user_comments_dict[post_user])\n",
    "        preprocessed_data['write_comment_num'][comment_user] += 1\n",
    "            \n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')\n",
    "print(\"\\npickling...\")\n",
    "\n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "preprocessed_data['word_freq'] = word_freq\n",
    "with open(\"pkl/preprocessed_bitcoin.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Number of unique vocabulary words\", len(voca))\n",
    "print(f\"Removed {n_removed_posts} posts\")\n",
    "print(f\"Removed {n_removed_comments} comments\")\n",
    "print()\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw_shell(preprocessed_data[\"user_network\"], with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "b = [len(a) for a in list(preprocessed_data['user_network'].adj.values())]\n",
    "print(max(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac5807ccc56803d53b6c0dc06a78365d93d8e4c11f3c7f359a83bbe035ca0794"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

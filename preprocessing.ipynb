{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:52.342511Z",
     "start_time": "2018-05-07T11:29:50.385716Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "'''\n",
    "Though not explicitly stated, I believe this \"stopwords\" package originates\n",
    "from this pip package: https://pypi.org/project/stop-words/\n",
    "\n",
    "It was the first result I found that shares the same name and get_stop_words\n",
    "function call. \n",
    "'''\n",
    "import stop_words\n",
    "from dateutil import parser\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# don't condense large numbers to scientific notation\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "filenames = glob(\"data/BTC/*.csv\")\n",
    "stopwords = set(stop_words.get_stop_words('en'))\n",
    "stopwords.update(['quote', 'pmquote', 'amquote', 'just', 'don', 'one', 'thing', 'even', 'way', 'maybe', 'also', 'please', 'well', 'actually', 'something',\n",
    "                                         'going', 'anything', 'le', 'ever', 'say', 'see', 'likely', 'per', 'another', 'someone', 'let', 'anyone', 'doesn', 'include', 'doe', 'exactly',\n",
    "                                         'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'like',\n",
    "                                         'said', 'guy', 'will', 'can', 'able', 'people', 'become', 'tell', 'hey', 'much', 'many', 'lol', 'lot', 'want', 'still', 'really', 'think', 'didn',\n",
    "                                         'isn', 'post', 'edited', 'share', 'facebookshare', 'twitter'])\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:54.235846Z",
     "start_time": "2018-05-07T11:29:54.225009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Current preprocessing protocol\n",
    "\n",
    "Remove:\n",
    "Hyperlinks\n",
    "Characters not in the English alphabetical character set a-z or A-Z\n",
    "stopwords\n",
    "words less than 3 characters\n",
    "'bitcoins' and replace with 'bitcoin'\n",
    "\n",
    "'''\n",
    "def parse_string(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    # remove links\n",
    "    input_string = re.sub(r'http\\S+', ' ', input_string)\n",
    "    input_string = re.sub(r'\\S+.(com|org)', '', input_string)\n",
    "    # remove all non-English alphabet characters including numbers,\n",
    "    # foreign and special characters\n",
    "    input_string = re.sub( \"[^a-zA-Z]\", \" \", input_string).split()\n",
    "\n",
    "    # lemmatize word\n",
    "    words = [lemmatizer.lemmatize(w) for w in input_string]\n",
    "    # get rid of stopwords and words less than 3 characters\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "    # change bitcoins to bitcoin\n",
    "    words = [w if w != 'bitcoins' else 'bitcoin' for w in words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:37:28.909112Z",
     "start_time": "2018-05-07T11:33:15.478609Z"
    }
   },
   "outputs": [],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "\n",
    "# a dictionary of user's posts, their time, quantity\n",
    "# accessed via preprocessed_data[field][user_name]\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts_num'] = defaultdict(int)\n",
    "preprocessed_data['get_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['write_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n",
    "\n",
    "\n",
    "fields = ['user_name', 'created_at', 'text', 'id', 'in_reply_to_screen_name']\n",
    "\n",
    "df = pd.concat([pd.read_csv(filename, nrows=None, usecols=fields) for filename in filenames])\n",
    "\n",
    "df = df[df['user_name'].notna()]\n",
    "df = df[df['created_at'].notna()]\n",
    "df = df[df['text'].notna()]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.sort_values(\"user_name\", inplace=True)\n",
    "\n",
    "if len(df[df.index.duplicated()]) > 0:\n",
    "    print(\"duplicate indices found! Something went wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% done"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Construct a new dataframe from the existing data in order to construct our digraph. The old\n",
    "code assumes that for each post, there is a number of comments directly linked to that post.\n",
    "For this dataset, however, each comment is linked to a user, which is still enough info to \n",
    "construct the graph, but does not allow us to process it in the same manner. \n",
    "\n",
    "The code below, therefore will link together user_name and a set of comments. \n",
    "Importantly, if the user the comments are directed towards doesn't exist, their total \n",
    "comments will be zero, as we can derive nothing from them. \n",
    "'''\n",
    "\n",
    "user_comments_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# create dictionary which maps usernames to a set of comment indices\n",
    "# start by looping through comments\n",
    "for index, user_name in df[\"in_reply_to_screen_name\"].items(): \n",
    "\n",
    "    if counter % 10000 == 0:\n",
    "        print(f\"\\r{counter/len(df) * 100:.2f}% done\", end='')\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    # skip nan values\n",
    "    if type(user_name) == float:\n",
    "        continue\n",
    "\n",
    "    user_loc = df['user_name'].searchsorted(user_name)\n",
    "\n",
    "    # the user this comment is pointing to doesn't have a post\n",
    "    if df.iloc[user_loc]['user_name'] != user_name:\n",
    "        continue\n",
    "\n",
    "    if user_name in user_comments_dict:\n",
    "        user_comments_dict[user_name].add(index)\n",
    "    #  user has comments, init and add it here\n",
    "    else:\n",
    "        user_comments_dict[user_name] = set([index])\n",
    "\n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_comments_dict\n",
    "# df['user_name'].searchsorted(\"#A'FaceBook'ThatPays\")\n",
    "\n",
    "# df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% done"
     ]
    }
   ],
   "source": [
    "word_freq_file_name = 'pkl/word_freq_pkl' \n",
    "\n",
    "# load cached result. Important: Make sure the number of tweets used is the same in the df!\n",
    "if os.path.exists(word_freq_file_name):\n",
    "    with open(word_freq_file_name, 'wb') as f:\n",
    "        word_freq = pickle.load(f)\n",
    "else:\n",
    "    # update word count, takes 8 mins or so\n",
    "    for i, text in enumerate(df[\"text\"]):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"\\r{i/len(df) * 100:.2f}% done\", end='')\n",
    "            \n",
    "        text_body = parse_string(text)\n",
    "        word_freq.update(text_body)\n",
    "\n",
    "    print(f\"\\r{100:.2f}% done\", end='')\n",
    "\n",
    "    with open(word_freq_file_name, 'wb') as f:\n",
    "        pickle.dump(word_freq, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% done\n",
      "41.92% edge utilization\n",
      "pickling...\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "user_processed_dict = {}\n",
    "\n",
    "counter = 0\n",
    "total_edges = sum([len(comments) for comments in user_comments_dict.values()])\n",
    "\n",
    "voca = set()\n",
    "\n",
    "min_post_len = 5\n",
    "min_comment_len = 5\n",
    "min_word_freq = 10\n",
    "\n",
    "total_len = len(df[df[\"in_reply_to_screen_name\"].isna()])\n",
    "\n",
    "# only loop through posts. Comments will be counted if the user is found\n",
    "# to be in the comments_user_dict \n",
    "# note: takes roughly 15 minutes on my machine\n",
    "for row in df[df[\"in_reply_to_screen_name\"].isna()].itertuples():\n",
    "\n",
    "    # absolute index, not implicit\n",
    "    index = row.Index\n",
    "\n",
    "    if counter % 5000 == 0:\n",
    "        print(f\"\\r{counter/total_len * 100:.2f}% done\", end='')\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    post_body = parse_string(row.text)\n",
    "    post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "    \n",
    "    # skip posts of four words or less\n",
    "    if len(post_body) < min_post_len:\n",
    "        continue\n",
    "\n",
    "    voca.update(post_body)\n",
    "    post_user = row.user_name\n",
    "    posted_time = parser.parse(row.created_at).date()\n",
    "    \n",
    "    preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "    preprocessed_data['user_posts_num'][post_user] += 1\n",
    "    preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "\n",
    "    # also add the same post data to a dictionary accessible by\n",
    "    # the user_name and time\n",
    "    if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "    else:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "    \n",
    "    preprocessed_data['posts'].append(post_body)\n",
    "\n",
    "    # skip any posts that don't have comments\n",
    "    if not(post_user in user_comments_dict):\n",
    "        continue\n",
    "\n",
    "    # comments already processed for this user\n",
    "    if post_user in user_processed_dict:\n",
    "        continue\n",
    "\n",
    "    # loop through the comments for the post and construct the digraph\n",
    "    for comment in df.loc[user_comments_dict[post_user]].itertuples():\n",
    "\n",
    "        user_processed_dict[post_user] = True\n",
    "\n",
    "        # skip comments on own post\n",
    "        if comment.user_name == post_user:\n",
    "            continue\n",
    "\n",
    "        comment_body = parse_string(comment.text)\n",
    "        comment_body = [w for w in comment_body if word_freq[w] >= min_word_freq]\n",
    "        \n",
    "        if len(comment_body) < min_comment_len:\n",
    "            continue\n",
    "\n",
    "        voca.update(comment_body)\n",
    "        comment_user = comment.user_name\n",
    "        comment_time = parser.parse(comment.created_at).date()\n",
    "            \n",
    "        preprocessed_data['user_posts'][comment_user].append(comment_body)\n",
    "        preprocessed_data['time_posts'][comment_time].append(comment_body)\n",
    "\n",
    "        if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "        else:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "        \n",
    "        preprocessed_data['posts'].append(comment_body)\n",
    "        preprocessed_data['user_network'].add_edge(comment_user, post_user)\n",
    "        preprocessed_data['get_comment_num'][post_user] = len(user_comments_dict[post_user])\n",
    "        preprocessed_data['write_comment_num'][comment_user] += 1\n",
    "            \n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')\n",
    "print(f\"\\n{100*preprocessed_data['user_network'].number_of_edges()/total_edges:.2f}% edge utilization\", end='')\n",
    "print(\"\\npickling...\")\n",
    "\n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "preprocessed_data['word_freq'] = word_freq\n",
    "with open(\"pkl/preprocessed_bitcoin.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw_shell(preprocessed_data[\"user_network\"], with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac5807ccc56803d53b6c0dc06a78365d93d8e4c11f3c7f359a83bbe035ca0794"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

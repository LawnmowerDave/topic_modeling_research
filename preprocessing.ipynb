{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:52.342511Z",
     "start_time": "2018-05-07T11:29:50.385716Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "'''\n",
    "Though not explicitly stated, I believe this \"stopwords\" package originates\n",
    "from this pip package: https://pypi.org/project/stop-words/\n",
    "\n",
    "It was the first result I found that shares the same name and get_stop_words\n",
    "function call. \n",
    "'''\n",
    "import stop_words\n",
    "from dateutil import parser\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from util import load, cache\n",
    "\n",
    "\n",
    "\n",
    "# don't condense large numbers to scientific notation\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "filenames = glob(\"data/BTC/*.csv\")\n",
    "stopwords = set(stop_words.get_stop_words('en'))\n",
    "stopwords.update(['quote', 'pmquote', 'amquote', 'just', 'don', 'one', 'thing', 'even', 'way', 'maybe', 'also', 'please', 'well', 'actually', 'something',\n",
    "                                         'going', 'anything', 'le', 'ever', 'say', 'see', 'likely', 'per', 'another', 'someone', 'let', 'anyone', 'doesn', 'include', 'doe', 'exactly',\n",
    "                                         'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'like',\n",
    "                                         'said', 'guy', 'will', 'can', 'able', 'people', 'become', 'tell', 'hey', 'much', 'many', 'lol', 'lot', 'want', 'still', 'really', 'think', 'didn',\n",
    "                                         'isn', 'post', 'edited', 'share', 'facebookshare', 'twitter'])\n",
    "                                        # adding exclusions to 'bitcion' because presumably, the entire dataset contains\n",
    "                                        # bitcoin-related tweets, which means mentions of bitcoin shouldn't add any value\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:54.235846Z",
     "start_time": "2018-05-07T11:29:54.225009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Current preprocessing protocol\n",
    "\n",
    "Remove:\n",
    "Hyperlinks\n",
    "Characters not in the English alphabetical character set a-z or A-Z\n",
    "stopwords\n",
    "words less than 3 characters\n",
    "'bitcoins' and replace with 'bitcoin'\n",
    "\n",
    "'''\n",
    "def parse_string(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    # remove links\n",
    "    input_string = re.sub(r'http\\S+', ' ', input_string)\n",
    "    input_string = re.sub(r'\\S+.(com|org)', '', input_string)\n",
    "    # remove all non-English alphabet characters including numbers,\n",
    "    # foreign and special characters\n",
    "    input_string = re.sub( \"[^a-zA-Z]\", \" \", input_string).split()\n",
    "\n",
    "    # lemmatize word\n",
    "    words = [lemmatizer.lemmatize(w) for w in input_string]\n",
    "    # get rid of stopwords and words less than 3 characters\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "\n",
    "    # fix common misspellings of bitcoin\n",
    "    words = [\"bitcoin\" if w == \"bitcoins\" else w for w in words]\n",
    "    words = [\"bitcoin\" if w == \"itcoin\" else w for w in words]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:37:28.909112Z",
     "start_time": "2018-05-07T11:33:15.478609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe size 4326298\n",
      "Number of posts 3503229\n",
      "Number of edges 823069\n"
     ]
    }
   ],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "\n",
    "# a dictionary of user's posts, their time, quantity\n",
    "# accessed via preprocessed_data[field][user_name]\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts_num'] = defaultdict(int)\n",
    "preprocessed_data['get_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['write_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n",
    "\n",
    "\n",
    "fields = ['user_name', 'created_at', 'text', 'id', 'in_reply_to_screen_name']\n",
    "\n",
    "df = pd.concat([pd.read_csv(filename, nrows=None, usecols=fields) for filename in filenames])\n",
    "\n",
    "# drop null usernames and text\n",
    "df = df[df['user_name'].notna()]\n",
    "df = df[df['created_at'].notna()]\n",
    "df = df[df['text'].notna()]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.sort_values(\"user_name\", inplace=True)\n",
    "\n",
    "# twitter usernames are not case sensitive so convert everything to lowercase\n",
    "df['user_name'] = df['user_name'].str.lower()\n",
    "df['in_reply_to_screen_name'] = df['in_reply_to_screen_name'].str.lower()\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "if len(df[df.index.duplicated()]) > 0:\n",
    "    print(\"duplicate indices found! Something went wrong.\")\n",
    "\n",
    "orig_n_edges = len(df[df['in_reply_to_screen_name'].notna()])\n",
    "orig_n_posts = len(df[df['in_reply_to_screen_name'].isna()])\n",
    "\n",
    "print(f\"dataframe size {len(df)}\")\n",
    "print(f\"Number of posts {orig_n_posts}\")\n",
    "print(f\"Number of edges {orig_n_edges}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 11.796376799393888\n",
      "median 1.0\n",
      "mode 1\n",
      "\n",
      "min 1\n",
      "max 43886\n"
     ]
    }
   ],
   "source": [
    "# show counts for number of posts for each user, some range in the thousands\n",
    "post_freq_dist = df[df['in_reply_to_screen_name'].isna()].groupby('user_name').count().sort_values('id')[\"index\"]\n",
    "\n",
    "# it appears that most nodes are identified by a single post\n",
    "print(f\"mean {post_freq_dist.mean()}\")\n",
    "print(f\"median {post_freq_dist.median()}\")\n",
    "print(f\"mode {post_freq_dist.mode()[0]}\")\n",
    "print()\n",
    "print(f\"min {post_freq_dist.min()}\")\n",
    "print(f\"max {post_freq_dist.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.85% done3503229\n",
      "100.00% done\n",
      "Number of posts 4326298\n",
      "Number of edges 52963\n",
      "\n",
      "Edge amount decreased by 93.57%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Construct a new dataframe from the existing data in order to construct our digraph. The old\n",
    "code assumes that for each post, there is a number of comments directly linked to that post.\n",
    "For this dataset, however, each comment is linked to a user, which is still enough info to \n",
    "construct the graph, but does not allow us to process it in the same manner. \n",
    "\n",
    "'''\n",
    "\n",
    "user_comments_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# create dictionary which maps usernames to a set of comment indices\n",
    "# start by looping through comments\n",
    "for index, user_name in df[\"in_reply_to_screen_name\"].items(): \n",
    "\n",
    "    if counter % 10000 == 0:\n",
    "        print(f\"\\r{counter/len(df) * 100:.2f}% done\", end='')\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    # skip nan values\n",
    "    if type(user_name) == float:\n",
    "        continue\n",
    "\n",
    "    user_loc = df['user_name'].searchsorted(user_name)\n",
    "\n",
    "    # the user this comment is pointing to doesn't have a post\n",
    "    if not(user_name in df.iloc[user_loc]['user_name']):\n",
    "        continue\n",
    "\n",
    "    if user_name in user_comments_dict:\n",
    "        user_comments_dict[user_name].append(index)\n",
    "    #  user has comments, init and add it here\n",
    "    else:\n",
    "        user_comments_dict[user_name] = [index]\n",
    "\n",
    "\n",
    "# drop all users without any comments pointing to them\n",
    "# df = df[df['user_name'].isin(user_comments_dict.keys())]\n",
    "\n",
    "# remove comments, as they're already listed in the user comments dict\n",
    "# df = df[df['in_reply_to_screen_name'].isna()]\n",
    "\n",
    "n_commented_posts = len(df)\n",
    "n_edges = sum([len(a) for a in user_comments_dict.values()])\n",
    "\n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')\n",
    "\n",
    "print()\n",
    "print(f\"Number of posts {n_commented_posts}\")\n",
    "print(f\"Number of edges {n_edges}\")\n",
    "print()\n",
    "print(f\"Edge amount decreased by {100*(1-n_edges/orig_n_edges):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts: 100.00% done\n",
      "Number of unique words: 569426\n"
     ]
    }
   ],
   "source": [
    "# word_freq_file_name = 'pkl/word_freq_pkl' \n",
    "\n",
    "# # load cached result. Important: Make sure the number of tweets used is the same in the df!\n",
    "# if os.path.exists(word_freq_file_name):\n",
    "#     with open(word_freq_file_name, 'rb') as f:\n",
    "#         word_freq = pickle.load(f)\n",
    "# else:\n",
    "\n",
    "#     with open(word_freq_file_name, 'wb') as f:\n",
    "#         pickle.dump(word_freq, f)\n",
    "\n",
    "# update word count\n",
    "for i, text in enumerate(df[\"text\"]):\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"\\rposts: {i/len(df) * 100:.2f}% done\", end='')\n",
    "        \n",
    "    text_body = parse_string(text)\n",
    "    word_freq.update(text_body)\n",
    "\n",
    "print(f\"\\rposts: {100:.2f}% done\", end='')\n",
    "print()\n",
    "print(\"Number of unique words:\", len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% done\n",
      "pickling...\n",
      "Number of unique vocabulary words 70401\n",
      "Removed 562045 posts\n",
      "Removed 0 comments\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "user_processed_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "voca = set()\n",
    "\n",
    "min_post_len = 5\n",
    "min_comment_len = 5\n",
    "min_word_freq = 10\n",
    "\n",
    "# counters for how many posts were removed from each source\n",
    "n_removed_posts = 0\n",
    "n_removed_comments = 0\n",
    "\n",
    "# only loop through posts. Comments will be counted if the user is found\n",
    "# to be in the comments_user_dict \n",
    "for row in df.itertuples():\n",
    "\n",
    "    # absolute index, not implicit\n",
    "    index = row.Index\n",
    "\n",
    "    if counter % 5000 == 0:\n",
    "        print(f\"\\r{counter/len(df) * 100:.2f}% done\", end='')\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    post_body = parse_string(row.text)\n",
    "    post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "    \n",
    "    # skip posts of four words or less\n",
    "    if len(post_body) < min_post_len:\n",
    "        n_removed_posts += 1\n",
    "        continue\n",
    "\n",
    "    voca.update(post_body)\n",
    "    post_user = row.user_name\n",
    "    posted_time = parser.parse(row.created_at).date()\n",
    "    \n",
    "    preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "    preprocessed_data['user_posts_num'][post_user] += 1\n",
    "    preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "\n",
    "    # also add the same post data to a dictionary accessible by\n",
    "    # the user_name and time\n",
    "    if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "    else:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "    \n",
    "    preprocessed_data['posts'].append(post_body)\n",
    "\n",
    "    # PROCSES COMMENTS FOR DIGRAPH\n",
    "\n",
    "    # comments already processed for this user\n",
    "    if post_user in user_processed_dict:\n",
    "        continue\n",
    "\n",
    "    # user not in comments dict, they are not apart of the digraph\n",
    "    if not(post_user) in user_comments_dict:\n",
    "        continue\n",
    "\n",
    "    # loop through the comments for the post and construct the digraph\n",
    "    for comment_index in user_comments_dict[post_user]:\n",
    "\n",
    "        comment = df.loc[comment_index]\n",
    "        user_processed_dict[post_user] = True\n",
    "\n",
    "        # skip comments on own post to avoid self-loops\n",
    "        if comment.user_name == post_user:\n",
    "            continue\n",
    "\n",
    "        preprocessed_data['user_network'].add_edge(comment['user_name'], post_user)\n",
    "            \n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')\n",
    "print(\"\\npickling...\")\n",
    "\n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "preprocessed_data['word_freq'] = word_freq\n",
    "with open(\"pkl/preprocessed_bitcoin.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Number of unique vocabulary words\", len(voca))\n",
    "print(f\"Removed {n_removed_posts} posts\")\n",
    "print(f\"Removed {n_removed_comments} comments\")\n",
    "print()\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw_shell(preprocessed_data[\"user_network\"], with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "b = [len(a) for a in list(preprocessed_data['user_network'].adj.values())]\n",
    "print(max(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac5807ccc56803d53b6c0dc06a78365d93d8e4c11f3c7f359a83bbe035ca0794"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

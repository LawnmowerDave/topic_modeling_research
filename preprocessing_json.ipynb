{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:52.342511Z",
     "start_time": "2018-05-07T11:29:50.385716Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "'''\n",
    "Though not explicitly stated, I believe this \"stopwords\" package originates\n",
    "from this pip package: https://pypi.org/project/stop-words/\n",
    "\n",
    "It was the first result I found that shares the same name and get_stop_words\n",
    "function call. \n",
    "'''\n",
    "import stop_words\n",
    "from dateutil import parser\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from util.util import load, cache\n",
    "from util.streamer import line_gen\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# don't condense large numbers to scientific notation\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "filenames = glob(\"data/BTC/json/*.jsonl\")\n",
    "stopwords = set(stop_words.get_stop_words('en'))\n",
    "stopwords.update(['quote', 'pmquote', 'amquote', 'just', 'don', 'one', 'thing', 'even', 'way', 'maybe', 'also', 'please', 'well', 'actually', 'something',\n",
    "                                         'going', 'anything', 'le', 'ever', 'say', 'see', 'likely', 'per', 'another', 'someone', 'let', 'anyone', 'doesn', 'include', 'doe', 'exactly',\n",
    "                                         'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'like',\n",
    "                                         'said', 'guy', 'will', 'can', 'able', 'people', 'become', 'tell', 'hey', 'much', 'many', 'lol', 'lot', 'want', 'still', 'really', 'think', 'didn',\n",
    "                                         'isn', 'post', 'edited', 'share', 'facebookshare', 'twitter', 'monday', 'tuedsday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])\n",
    "                                        # adding exclusions to 'bitcion' because presumably, the entire dataset contains\n",
    "                                        # bitcoin-related tweets, which means mentions of bitcoin shouldn't add any value\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:54.235846Z",
     "start_time": "2018-05-07T11:29:54.225009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_string(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    # remove links\n",
    "    input_string = re.sub(r'http\\S+', ' ', input_string)\n",
    "    input_string = re.sub(r'\\S+.(com|org)', '', input_string)\n",
    "    # remove all non-English alphabet characters including numbers,\n",
    "    # foreign and special characters\n",
    "    input_string = re.sub( \"[^a-zA-Z]\", \" \", input_string).split()\n",
    "\n",
    "    # lemmatize word\n",
    "    words = [lemmatizer.lemmatize(w) for w in input_string]\n",
    "    # get rid of stopwords and words less than 3 characters\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "\n",
    "    # fix common misspellings of bitcoin\n",
    "    words = [\"bitcoin\" if w == \"bitcoins\" else w for w in words]\n",
    "    words = [\"bitcoin\" if w == \"itcoin\" else w for w in words]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:37:28.909112Z",
     "start_time": "2018-05-07T11:33:15.478609Z"
    }
   },
   "outputs": [],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "\n",
    "# a dictionary of user's posts, their time, quantity\n",
    "# accessed via preprocessed_data[field][user_name]\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts_num'] = defaultdict(int)\n",
    "preprocessed_data['get_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['write_comment_num'] = defaultdict(int)\n",
    "\n",
    "# various Twitter metadata\n",
    "preprocessed_data['verified'] = defaultdict(bool)\n",
    "preprocessed_data['retweeted'] = defaultdict(bool)\n",
    "\n",
    "preprocessed_data['retweet_count'] = defaultdict(int)\n",
    "preprocessed_data['followers_count'] = defaultdict(int)\n",
    "\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 400,000 tweets is the max my machine can load into RAM (total dataset is about 16 M)\n",
    "# df = pd.concat([pd.read_json(filename, nrows=n_tweets/len(filenames), lines=True) for filename in filenames])\n",
    "df = load('btc_data')\n",
    "\n",
    "df = df.sort_values(by='in_reply_to_user_id')\n",
    "\n",
    "# convert strings to datetime objects\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts:  3578229\n",
      "comments:  700308\n"
     ]
    }
   ],
   "source": [
    "print('posts: ', len(df[df['in_reply_to_user_id'].isna()]))\n",
    "print('comments: ', len(df[df['in_reply_to_user_id'].notna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_counts = {}\n",
    "\n",
    "# for row in df.itertuples():\n",
    "#     if not row.created_at.year in date_counts:\n",
    "#         date_counts[row.created_at.year] = 1\n",
    "#     else:\n",
    "#         date_counts[row.created_at.year] += 1\n",
    "\n",
    "# print(date_counts)\n",
    "# plt.plot(sorted(date_counts.keys()), sorted(date_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['full_text', 'created_at', 'in_reply_to_user_id', 'retweet_count',\n",
       "       'retweeted', 'id', 'verified', 'followers_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts: 100.00% done\n",
      "Number of unique words: 833141\n"
     ]
    }
   ],
   "source": [
    "# word_freq_file_name = 'pkl/word_freq_pkl' \n",
    "\n",
    "# # load cached result. Important: Make sure the number of tweets used is the same in the df!\n",
    "# if os.path.exists(word_freq_file_name):\n",
    "#     with open(word_freq_file_name, 'rb') as f:\n",
    "#         word_freq = pickle.load(f)\n",
    "# else:\n",
    "\n",
    "#     with open(word_freq_file_name, 'wb') as f:\n",
    "#         pickle.dump(word_freq, f)\n",
    "\n",
    "# update word count\n",
    "for i, text in enumerate(df[\"full_text\"]):\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"\\rposts: {i/len(df) * 100:.2f}% done\", end='')\n",
    "        \n",
    "    text_body = text\n",
    "    word_freq.update(text_body)\n",
    "\n",
    "print(f\"\\rposts: {100:.2f}% done\", end='')\n",
    "print()\n",
    "print(\"Number of unique words:\", len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 91353 nodes and 281212 edges"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_191830/3411409762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mpost_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'in_reply_to_user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming/topic_modeling_research/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3447\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming/topic_modeling_research/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3501\u001b[0m         \u001b[0;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_processed_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "voca = set()\n",
    "\n",
    "# only loop through posts. Comments will be counted if the user is found\n",
    "# to be in the comments_user_dict \n",
    "for i, post in enumerate(df[df['in_reply_to_user_id'].isna()].itertuples()):\n",
    "\n",
    "    post_user = post.id\n",
    "\n",
    "    # add metadata\n",
    "    preprocessed_data['verified'][post_user] = post.verified\n",
    "    preprocessed_data['followers_count'][post_user] = post.followers_count\n",
    "    preprocessed_data['retweeted'][post_user] = post.retweeted\n",
    "    preprocessed_data['retweet_count'][post_user] = post.retweet_count\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"\\r{i/len(df[df['in_reply_to_user_id'].isna()]) * 100:.2f}% done\", end='')\n",
    "        print(f\"\\r{preprocessed_data['user_network']}\", end='')\n",
    "\n",
    "\n",
    "    post_body = post.full_text\n",
    "    post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "    \n",
    "    if len(post_body) < 5:\n",
    "        continue\n",
    "\n",
    "    voca.update(post_body)\n",
    "    \n",
    "    posted_time = post.created_at\n",
    "    \n",
    "    preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "    preprocessed_data['user_posts_num'][post_user] += 1\n",
    "    preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "    if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "    else:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "        \n",
    "    preprocessed_data['posts'].append(post_body)\n",
    "\n",
    "    if str(post_user) in user_processed_dict:\n",
    "        continue\n",
    "\n",
    "    post_comments = df[df['in_reply_to_user_id'] == post.id]\n",
    "    \n",
    "    for comment in post_comments.itertuples():\n",
    "\n",
    "        user_processed_dict[str(post_user)] = True\n",
    "\n",
    "        comment_body = comment.full_text\n",
    "\n",
    "        comment_body = [w for w in comment_body if word_freq[w] >= 10]\n",
    "        if len(comment_body) < 5:\n",
    "            continue\n",
    "        voca.update(comment_body)\n",
    "        comment_user = comment.id\n",
    "        comment_time = comment.created_at\n",
    "            \n",
    "        preprocessed_data['user_posts'][comment_user].append(comment_body)\n",
    "        preprocessed_data['time_posts'][comment_time].append(comment_body)\n",
    "        if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "        else:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "        \n",
    "        preprocessed_data['posts'].append(comment_body)\n",
    "        preprocessed_data['user_network'].add_edge(comment_user, post_user)\n",
    "        preprocessed_data['get_comment_num'][post_user] += 1\n",
    "        preprocessed_data['write_comment_num'][comment_user] += 1\n",
    "        \n",
    "\n",
    "print(f\"\\r{100:.2f}% done\", end='')\n",
    "print(\"\\npickling...\")\n",
    "\n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "preprocessed_data['word_freq'] = word_freq\n",
    "with open(\"pkl/preprocessed_bitcoin.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Number of unique vocabulary words\", len(voca))\n",
    "print()\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1077871"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3578229"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['in_reply_to_user_id'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 61442 nodes and 180474 edges\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_data['user_network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw_shell(preprocessed_data[\"user_network\"], with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# b = [len(a) for a in list(preprocessed_data['user_network'].adj.values())]\n",
    "# print(max(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac5807ccc56803d53b6c0dc06a78365d93d8e4c11f3c7f359a83bbe035ca0794"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
